{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "data = load_dataset(\"sentence-transformers/squad\", split=\"train\")\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "reranker_model = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-2-v2\", max_length=512)\n",
    "api_key = os.getenv(\"CLARIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 18891\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.filter(\n",
    "    lambda row, unique=set(): not (row[\"answer\"] in unique or unique.add(row[\"answer\"]))\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"question\"]\n",
    "answers = data[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/answer_embeddings.pt\"):\n",
    "    answer_embeddings = model.encode(\n",
    "        answers, convert_to_tensor=True, batch_size=64, show_progress_bar=True\n",
    "    )\n",
    "    torch.save(answer_embeddings, \"../data/answer_embeddings.pt\")\n",
    "else:\n",
    "    answer_embeddings = torch.load(\"../data/answer_embeddings.pt\", weights_only=True)\n",
    "\n",
    "if not os.path.exists(\"../data/question_embeddings.pt\"):\n",
    "    question_embeddings = model.encode(\n",
    "        questions, convert_to_tensor=True, batch_size=64, show_progress_bar=True\n",
    "    )\n",
    "    torch.save(question_embeddings, \"../data/question_embeddings.pt\")\n",
    "else:\n",
    "    question_embeddings = torch.load(\"../data/question_embeddings.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query_top_k(query, corpus_embedding, corpus, top_k=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = model.similarity(query_embedding, corpus_embedding)[0]\n",
    "    scores, indices = torch.topk(cosine_scores, k=top_k)\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Top 5 answers:\")\n",
    "    for i, index in enumerate(indices):\n",
    "        print(f\"\\t{i+1}) {corpus[index]}\")\n",
    "        print(\"Cosine similarity score:\", scores[i].item())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What retired NASA Office of inspector general is outspoken about the FAA?\n",
      "Top 5 answers:\n",
      "\t1) The FAA has been cited as an example of regulatory capture, \"in which the airline industry openly dictates to its regulators its governing rules, arranging for not only beneficial regulation, but placing key people to head these regulators.\" Retired NASA Office of Inspector General Senior Special Agent Joseph Gutheinz, who used to be a Special Agent with the Office of Inspector General for the Department of Transportation and with FAA Security, is one of the most outspoken critics of FAA. Rather than commend the agency for proposing a $10.2 million fine against Southwest Airlines for its failure to conduct mandatory inspections in 2008, he was quoted as saying the following in an Associated Press story: \"Penalties against airlines that violate FAA directives should be stiffer. At $25,000 per violation, Gutheinz said, airlines can justify rolling the dice and taking the chance on getting caught. He also said the FAA is often too quick to bend to pressure from airlines and pilots.\" Other experts have been critical of the constraints and expectations under which the FAA is expected to operate. The dual role of encouraging aerospace travel and regulating aerospace travel are contradictory. For example, to levy a heavy penalty upon an airline for violating an FAA regulation which would impact their ability to continue operating would not be considered encouraging aerospace travel.\n",
      "Cosine similarity score: 24.608619689941406\n",
      "\n",
      "\t2) In 2007, two FAA whistleblowers, inspectors Charalambe \"Bobby\" Boutris and Douglas E. Peters, alleged that Boutris said he attempted to ground Southwest after finding cracks in the fuselage, but was prevented by supervisors he said were friendly with the airline. This was validated by a report by the Department of Transportation which found FAA managers had allowed Southwest Airlines to fly 46 airplanes in 2006 and 2007 that were overdue for safety inspections, ignoring concerns raised by inspectors. Audits of other airlines resulted in two airlines grounding hundreds of planes, causing thousands of flight cancellations. The House Transportation and Infrastructure Committee held hearings in April 2008. Jim Oberstar, former chairman of the committee said its investigation uncovered a pattern of regulatory abuse and widespread regulatory lapses, allowing 117 aircraft to be operated commercially although not in compliance with FAA safety rules. Oberstar said there was a \"culture of coziness\" between senior FAA officials and the airlines and \"a systematic breakdown\" in the FAA's culture that resulted in \"malfeasance, bordering on corruption.\" In 2008 the FAA proposed to fine Southwest $10.2 million for failing to inspect older planes for cracks, and in 2009 Southwest and the FAA agreed that Southwest would pay a $7.5 million penalty and would adopt new safety procedures, with the fine doubling if Southwest failed to follow through.\n",
      "Cosine similarity score: 19.918292999267578\n",
      "\n",
      "\t3) Feynman devoted the latter half of his book What Do You Care What Other People Think? to his experience on the Rogers Commission, straying from his usual convention of brief, light-hearted anecdotes to deliver an extended and sober narrative. Feynman's account reveals a disconnect between NASA's engineers and executives that was far more striking than he expected. His interviews of NASA's high-ranking managers revealed startling misunderstandings of elementary concepts. For instance, NASA managers claimed that there was a 1 in 100,000 chance of a catastrophic failure aboard the shuttle, but Feynman discovered that NASA's own engineers estimated the chance of a catastrophe at closer to 1 in 200. He concluded that the space shuttle reliability estimate by NASA management was fantastically unrealistic, and he was particularly angered that NASA used these figures to recruit Christa McAuliffe into the Teacher-in-Space program. He warned in his appendix to the commission's report (which was included only after he threatened not to sign the report), \"For a successful technology, reality must take precedence over public relations, for nature cannot be fooled.\"\n",
      "Cosine similarity score: 18.80128288269043\n",
      "\n",
      "\t4) Notable alumni include: Alan Mulally (BS/MS), former President and CEO of Ford Motor Company, Lou Montulli, co-founder of Netscape and author of the Lynx web browser, Brian McClendon (BSEE 1986), VP of Engineering at Google, Charles E. Spahr (1934), former CEO of Standard Oil of Ohio.\n",
      "Cosine similarity score: 18.739206314086914\n",
      "\n",
      "\t5) The Office of Government Information Services (OGIS) is a Freedom of Information Act (FOIA) resource for the public and the government. Congress has charged NARA with reviewing FOIA policies, procedures and compliance of Federal agencies and to recommend changes to FOIA. NARA's mission also includes resolving FOIA disputes between Federal agencies and requesters.\n",
      "Cosine similarity score: 18.714401245117188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_question = random.choice(questions)\n",
    "search_query_top_k(random_question, answer_embeddings, answers, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/answers_dataset.pt\"):\n",
    "    answers_dataset = Dataset.from_dict(\n",
    "        {\"answers\": data[\"answer\"], \"embeddings\": answer_embeddings.cpu().numpy()}\n",
    "    )\n",
    "    answers_dataset = answers_dataset.map(lambda x: {\"embeddings\": normalize(x[\"embeddings\"])})\n",
    "\n",
    "    answers_dataset = answers_dataset.add_faiss_index(\n",
    "        column=\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT\n",
    "    )\n",
    "    answers_dataset.save_faiss_index(\"embeddings\", \"../data/answer_embeddings_faiss\")\n",
    "    torch.save(answers_dataset, \"../data/answers_dataset.pt\")\n",
    "else:\n",
    "    answers_dataset = torch.load(\"../data/answers_dataset.pt\", weights_only=False)\n",
    "    answers_dataset.load_faiss_index(\"embeddings\", \"../data/answer_embeddings_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/questions_dataset.pt\"):\n",
    "    questions_dataset = Dataset.from_dict(\n",
    "        {\"questions\": data[\"question\"], \"embeddings\": question_embeddings.cpu().numpy()}\n",
    "    )\n",
    "    questions_dataset = questions_dataset.map(lambda x: {\"embeddings\": normalize(x[\"embeddings\"])})\n",
    "\n",
    "    questions_dataset = questions_dataset.add_faiss_index(\n",
    "        column=\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT\n",
    "    )\n",
    "    questions_dataset.save_faiss_index(\"embeddings\", \"../data/question_embeddings_faiss\")\n",
    "    torch.save(questions_dataset, \"../data/questions_dataset.pt\")\n",
    "else:\n",
    "    questions_dataset = torch.load(\"../data/questions_dataset.pt\", weights_only=False)\n",
    "    answers_dataset.load_faiss_index(\"embeddings\", \"../data/question_embeddings_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query_top_k_faiss(query, answers_ds, corpus, top_k=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "    query_embedding = normalize(query_embedding)\n",
    "\n",
    "    scores, samples = answers_ds.get_nearest_examples(\"embeddings\", query_embedding, k=top_k)\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Top 5 answers:\")\n",
    "    for i, row in samples_df.iterrows():\n",
    "        print(f\"\\t{i+1}) {row['answers']}\")\n",
    "        print(\"Cosine similarity score:\", row[\"scores\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Top 5 answers:\n",
      "\t1) Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Cosine similarity score: 1.0\n",
      "\n",
      "\t2) The Gospel of Luke begins its account of Mary's life with the Annunciation, when the angel Gabriel appeared to her and announced her divine selection to be the mother of Jesus. According to gospel accounts, Mary was present at the Crucifixion of Jesus and is depicted as a member of the early Christian community in Jerusalem. According to Apocryphal writings, at some time soon after her death, her incorrupt body was assumed directly into Heaven, to be reunited with her soul, and the apostles thereupon found the tomb empty; this is known in Christian teaching as the Assumption.\n",
      "Cosine similarity score: 0.7234536409378052\n",
      "\n",
      "\t3) Ephesus is a cultic centre of Mary, the site of the first Church dedicated to her and the rumoured place of her death. Ephesus was previously a centre for worship of Artemis a virgin goddess. The Temple of Artemis at Ephesus being regarded as one of the Seven Wonders of the Ancient World The cult of Mary was furthered by Queen Theodora in the 6th Century. According to William E. Phipps, in the book Survivals of Roman Religion \"Gordon Laing argues convincingly that the worship of Artemis as both virgin and mother at the grand Ephesian temple contributed to the veneration of Mary.\"\n",
      "Cosine similarity score: 0.6925266981124878\n",
      "\n",
      "\t4) The popularity of this particular representation of The Immaculate Conception spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The moon is under her feet and a halo of twelve stars surround her head, possibly a reference to \"a woman clothed with the sun\" from Revelation 12:1-2. Additional imagery may include clouds, a golden light, and cherubs. In some paintings the cherubim are holding lilies and roses, flowers often associated with Mary.\n",
      "Cosine similarity score: 0.6836040616035461\n",
      "\n",
      "\t5) Some Western writers claim that the immaculate conception of Mary is a teaching of Islam. Thus, commenting in 1734 on the passage in the Qur'an, \"I have called her Mary; and I commend her to thy protection, and also her issue, against Satan driven away with stones\", George Sale stated: \"It is not improbable that the pretended immaculate conception of the virgin Mary is intimated in this passage. For according to a tradition of Mohammed, every person that comes into the world, is touched at his birth by the devil, and therefore cries out, Mary and her son only excepted; between whom, and the evil spirit God placed a veil, so that his touch did not reach them. And for this reason they say, neither of them were guilty of any sin, like the rest of the children of Adam.\"\n",
      "Cosine similarity score: 0.6746484041213989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query_top_k_faiss(questions[0], answers_dataset, answers, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: At what New Haven museum is an original copy of the Gutenberg Bible housed?\n",
      "Top 5 answers:\n",
      "\t1) New Haven has a variety of museums, many of them associated with Yale. The Beinecke Rare Book and Manuscript Library features an original copy of the Gutenberg Bible. There is also the Connecticut Children's Museum; the Knights of Columbus museum near that organization's world headquarters; the Peabody Museum of Natural History; the Yale University Collection of Musical Instruments; the Eli Whitney Museum (across the town line in Hamden, Connecticut, on Whitney Avenue); the Yale Center for British Art, which houses the largest collection of British art outside the U.K., and the Yale University Art Gallery, the nation's oldest college art museum.[citation needed] New Haven is also home to the New Haven Museum and Historical Society on Whitney Avenue, which has a library of many primary source treasures dating from Colonial times to the present.\n",
      "Cosine similarity score: 1.0\n",
      "\n",
      "\t2) Grove Street Cemetery, a National Historic Landmark which lies adjacent to Yale's campus, contains the graves of Roger Sherman, Eli Whitney, Noah Webster, Josiah Willard Gibbs, Charles Goodyear and Walter Camp, among other notable burials. The cemetery is known for its grand Egyptian Revival gateway. The Union League Club of New Haven building, located on Chapel Street, is notable for not only being a historic Beaux-Arts building, but also is built on the site where Roger Sherman's home once stood; George Washington is known to have stayed at the Sherman residence while President in 1789 (one of three times Washington visited New Haven throughout his lifetime).\n",
      "Cosine similarity score: 0.6949927806854248\n",
      "\n",
      "\t3) The National Archives Building in downtown Washington holds record collections such as all existing federal census records, ships' passenger lists, military unit records from the American Revolution to the Philippineâ€“American War, records of the Confederate government, the Freedmen's Bureau records, and pension and land records.\n",
      "Cosine similarity score: 0.6792083978652954\n",
      "\n",
      "\t4) Artspace on Orange Street is one of several contemporary art galleries around the city, showcasing the work of local, national, and international artists. Others include City Gallery and A. Leaf Gallery in the downtown area. Westville galleries include Kehler Liddell, Jennifer Jane Gallery, and The Hungry Eye. The Erector Square complex in the Fair Haven neighborhood houses the Parachute Factory gallery along with numerous artist studios, and the complex serves as an active destination during City-Wide Open Studios held yearly in October.\n",
      "Cosine similarity score: 0.6580445170402527\n",
      "\n",
      "\t5) In addition to the Jazz Festival (described above), New Haven serves as the home city of the annual International Festival of Arts and Ideas. New Haven's Saint Patrick's Day parade, which began in 1842, is New England's oldest St. Patty's Day parade and draws the largest crowds of any one-day spectator event in Connecticut. The St. Andrew the Apostle Italian Festival has taken place in the historic Wooster Square neighborhood every year since 1900. Other parishes in the city celebrate the Feast of Saint Anthony of Padua and a carnival in honor of St. Bernadette Soubirous. New Haven celebrates Powder House Day every April on the New Haven Green to commemorate the city's entrance into the Revolutionary War. The annual Wooster Square Cherry Blossom Festival commemorates the 1973 planting of 72 Yoshino Japanese Cherry Blossom trees by the New Haven Historic Commission in collaboration with the New Haven Parks Department and residents of the neighborhood. The Festival now draws well over 5,000 visitors. The Film Fest New Haven has been held annually since 1995.\n",
      "Cosine similarity score: 0.652753472328186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_question = random.choice(questions)\n",
    "search_query_top_k_faiss(random_question, answers_dataset, answers, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    query: str,\n",
    "    answer_embeddings,\n",
    "    answers_dataset,\n",
    "    top_k: int = 5,\n",
    "    faiss: bool = False,\n",
    "    answers_key: str = \"answers\",\n",
    "):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "    if faiss:\n",
    "        scores, samples = answers_dataset.get_nearest_examples(\n",
    "            \"embeddings\", query_embedding, k=top_k\n",
    "        )\n",
    "        retrieved_indices = [\n",
    "            answers_dataset[answers_key].index(answer) for answer in samples[answers_key]\n",
    "        ]\n",
    "    else:\n",
    "        cosine_scores = model.similarity(query_embedding, answer_embeddings.cpu())[0]\n",
    "        scores, indices = torch.topk(cosine_scores, k=top_k)\n",
    "        retrieved_indices = indices.tolist()\n",
    "\n",
    "    return scores, retrieved_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(relevant_index, retrieved_indices):\n",
    "    recall = 1 if relevant_index in retrieved_indices else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_questions = questions[:1000]\n",
    "recall_cosine_search = []\n",
    "recall_faiss_search = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall (Cosine Search): 0.913 Time taken: 20.993836402893066\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for query in subset_questions:\n",
    "    relevant_index = questions.index(query)\n",
    "    scores, retrieved_indices = search(\n",
    "        query, answer_embeddings, answers_dataset, top_k=5, faiss=False\n",
    "    )\n",
    "    recall_cosine_search.append(calculate_recall(relevant_index, retrieved_indices))\n",
    "average_recall_semantic_search = sum(recall_cosine_search) / len(recall_cosine_search)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\n",
    "    \"Average Recall (Cosine Search):\",\n",
    "    average_recall_semantic_search,\n",
    "    \"Time taken:\",\n",
    "    end_time - start_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall (Faiss Search): 1.0 Time taken: 178.97983503341675\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for query in subset_questions:\n",
    "    relevant_index = questions.index(query)\n",
    "    scores, retrieved_indices = search(\n",
    "        query, answer_embeddings, answers_dataset, top_k=5, faiss=True\n",
    "    )\n",
    "    recall_faiss_search.append(calculate_recall(relevant_index, retrieved_indices))\n",
    "average_recall_faiss_search = sum(recall_faiss_search) / len(recall_faiss_search)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\n",
    "    \"Average Recall (Faiss Search):\",\n",
    "    average_recall_faiss_search,\n",
    "    \"Time taken:\",\n",
    "    end_time - start_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.7784666666666672 Time taken: 21.9837327003479\n",
      "Mean Reciprocal Rank (MRR) using FAISS: 0.9985 Time taken: 180.64294600486755\n"
     ]
    }
   ],
   "source": [
    "def calculate_mrr(queries, questions, answer_embeddings, top_k=5, faiss=False):\n",
    "    mrr_total = 0.0\n",
    "\n",
    "    for query in queries:\n",
    "        relevant_index = questions.index(query)\n",
    "        _, retrieved_indices = search(\n",
    "            query, answer_embeddings, answers_dataset, top_k=top_k, faiss=faiss\n",
    "        )\n",
    "\n",
    "        if relevant_index in retrieved_indices:\n",
    "            rank = retrieved_indices.index(relevant_index) + 1\n",
    "            reciprocal_rank = 1 / rank\n",
    "        else:\n",
    "            reciprocal_rank = 0\n",
    "\n",
    "        mrr_total += reciprocal_rank\n",
    "\n",
    "    mrr = mrr_total / len(queries)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "mrr = calculate_mrr(subset_questions, questions, answer_embeddings, top_k=5)\n",
    "end_time = time.time()\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr, \"Time taken:\", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "mrr_faiss = calculate_mrr(subset_questions, questions, answers_dataset, top_k=5, faiss=True)\n",
    "end_time = time.time()\n",
    "print(\"Mean Reciprocal Rank (MRR) using FAISS:\", mrr_faiss, \"Time taken:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, retrieved_indices, corpus, verbose=False):\n",
    "    retrieved_answers = [corpus[idx] for idx in retrieved_indices]\n",
    "\n",
    "    pairs = [(query, answer) for answer in retrieved_answers]\n",
    "\n",
    "    reranked_scores = reranker_model.predict(pairs)\n",
    "    sort_indices = np.argsort(reranked_scores)[::-1]\n",
    "    reranked_indices = [retrieved_indices[i] for i in sort_indices]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Query:\", query)\n",
    "        print(\"Top 5 answers after reranking:\")\n",
    "        for i, index in enumerate(reranked_indices):\n",
    "            print(f\"\\t{i+1}) {corpus[index]}\")\n",
    "            print(\"Reranked score:\", reranked_scores[sort_indices[i]])\n",
    "            print()\n",
    "\n",
    "    return reranked_scores, reranked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RERANK WITHOUT FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Top 5 answers after reranking:\n",
      "\t1) Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Reranked score: -1.5392361\n",
      "\n",
      "\t2) Henry VII added a Perpendicular style chapel dedicated to the Blessed Virgin Mary in 1503 (known as the Henry VII Chapel or the \"Lady Chapel\"). Much of the stone came from Caen, in France (Caen stone), the Isle of Portland (Portland stone) and the Loire Valley region of France (tuffeau limestone).[citation needed]\n",
      "Reranked score: -5.720572\n",
      "\n",
      "\t3) It seems to have been St Bernard of Clairvaux who, in the 12th century, explicitly raised the question of the Immaculate Conception. A feast of the Conception of the Blessed Virgin had already begun to be celebrated in some churches of the West. St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See. In doing so, he takes occasion to repudiate altogether the view that the conception of Mary was sinless. It is doubtful, however, whether he was using the term \"conception\" in the same sense in which it is used in the definition of Pope Pius IX. Bernard would seem to have been speaking of conception in the active sense of the mother's cooperation, for in his argument he says: \"How can there be absence of sin where there is concupiscence (libido)?\" and stronger expressions follow, showing that he is speaking of the mother and not of the child.\n",
      "Reranked score: -8.423656\n",
      "\n",
      "\t4) The Gospel of Luke begins its account of Mary's life with the Annunciation, when the angel Gabriel appeared to her and announced her divine selection to be the mother of Jesus. According to gospel accounts, Mary was present at the Crucifixion of Jesus and is depicted as a member of the early Christian community in Jerusalem. According to Apocryphal writings, at some time soon after her death, her incorrupt body was assumed directly into Heaven, to be reunited with her soul, and the apostles thereupon found the tomb empty; this is known in Christian teaching as the Assumption.\n",
      "Reranked score: -9.024513\n",
      "\n",
      "\t5) In paintings, Mary is traditionally portrayed in blue. This tradition can trace its origin to the Byzantine Empire, from c.500 AD, where blue was \"the colour of an empress\". A more practical explanation for the use of this colour is that in Medieval and Renaissance Europe, the blue pigment was derived from the rock lapis lazuli, a stone imported from Afghanistan of greater value than gold. Beyond a painter's retainer, patrons were expected to purchase any gold or lapis lazuli to be used in the painting. Hence, it was an expression of devotion and glorification to swathe the Virgin in gowns of blue.\n",
      "Reranked score: -9.904217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, retrieved_indices = search(\n",
    "    questions[0], answer_embeddings, answers_dataset, top_k=5, faiss=False\n",
    ")\n",
    "\n",
    "rerank(questions[0], retrieved_indices, answers, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RERANK WITH FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Top 5 answers after reranking:\n",
      "\t1) Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Reranked score: -1.5392375\n",
      "\n",
      "\t2) Some Western writers claim that the immaculate conception of Mary is a teaching of Islam. Thus, commenting in 1734 on the passage in the Qur'an, \"I have called her Mary; and I commend her to thy protection, and also her issue, against Satan driven away with stones\", George Sale stated: \"It is not improbable that the pretended immaculate conception of the virgin Mary is intimated in this passage. For according to a tradition of Mohammed, every person that comes into the world, is touched at his birth by the devil, and therefore cries out, Mary and her son only excepted; between whom, and the evil spirit God placed a veil, so that his touch did not reach them. And for this reason they say, neither of them were guilty of any sin, like the rest of the children of Adam.\"\n",
      "Reranked score: -8.247177\n",
      "\n",
      "\t3) Ephesus is a cultic centre of Mary, the site of the first Church dedicated to her and the rumoured place of her death. Ephesus was previously a centre for worship of Artemis a virgin goddess. The Temple of Artemis at Ephesus being regarded as one of the Seven Wonders of the Ancient World The cult of Mary was furthered by Queen Theodora in the 6th Century. According to William E. Phipps, in the book Survivals of Roman Religion \"Gordon Laing argues convincingly that the worship of Artemis as both virgin and mother at the grand Ephesian temple contributed to the veneration of Mary.\"\n",
      "Reranked score: -8.345813\n",
      "\n",
      "\t4) The Gospel of Luke begins its account of Mary's life with the Annunciation, when the angel Gabriel appeared to her and announced her divine selection to be the mother of Jesus. According to gospel accounts, Mary was present at the Crucifixion of Jesus and is depicted as a member of the early Christian community in Jerusalem. According to Apocryphal writings, at some time soon after her death, her incorrupt body was assumed directly into Heaven, to be reunited with her soul, and the apostles thereupon found the tomb empty; this is known in Christian teaching as the Assumption.\n",
      "Reranked score: -9.024513\n",
      "\n",
      "\t5) The popularity of this particular representation of The Immaculate Conception spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The moon is under her feet and a halo of twelve stars surround her head, possibly a reference to \"a woman clothed with the sun\" from Revelation 12:1-2. Additional imagery may include clouds, a golden light, and cherubs. In some paintings the cherubim are holding lilies and roses, flowers often associated with Mary.\n",
      "Reranked score: -10.4344635\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, retrieved_indices = search(\n",
    "    questions[0], answer_embeddings, answers_dataset, top_k=5, faiss=True\n",
    ")\n",
    "\n",
    "rerank(questions[0], retrieved_indices, answers, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall with Reranking (Cosine Search): 0.913\n",
      "Average Recall with Reranking (FAISS Search): 1.0\n"
     ]
    }
   ],
   "source": [
    "recall_rerank_cosine_search = []\n",
    "recall_rerank_faiss_search = []\n",
    "\n",
    "for query in subset_questions:\n",
    "    relevant_index = questions.index(query)\n",
    "\n",
    "    scores, retrieved_indices = search(\n",
    "        query, answer_embeddings, answers_dataset, top_k=5, faiss=False\n",
    "    )\n",
    "    reranked_scores, reranked_indices = rerank(query, retrieved_indices, answers)\n",
    "    recall_rerank_cosine_search.append(calculate_recall(relevant_index, reranked_indices))\n",
    "\n",
    "    scores, retrieved_indices = search(\n",
    "        query, answer_embeddings, answers_dataset, top_k=5, faiss=True\n",
    "    )\n",
    "    reranked_scores, reranked_indices = rerank(query, retrieved_indices, answers)\n",
    "    recall_rerank_faiss_search.append(calculate_recall(relevant_index, reranked_indices))\n",
    "\n",
    "average_recall_rerank_cosine_search = sum(recall_rerank_cosine_search) / len(\n",
    "    recall_rerank_cosine_search\n",
    ")\n",
    "average_recall_rerank_faiss_search = sum(recall_rerank_faiss_search) / len(\n",
    "    recall_rerank_faiss_search\n",
    ")\n",
    "\n",
    "print(\"Average Recall with Reranking (Cosine Search):\", average_recall_rerank_cosine_search)\n",
    "print(\"Average Recall with Reranking (FAISS Search):\", average_recall_rerank_faiss_search)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR) with Reranking (Cosine Search): 0.8163666666666667\n",
      "Mean Reciprocal Rank (MRR) with Reranking (FAISS Search): 0.9087000000000003\n"
     ]
    }
   ],
   "source": [
    "def calculate_mrr_rerank(queries, questions, answer_embeddings, top_k=5, faiss=False):\n",
    "    mrr_total = 0.0\n",
    "\n",
    "    for query in queries:\n",
    "        relevant_index = questions.index(query)\n",
    "        _, retrieved_indices = search(\n",
    "            query, answer_embeddings, answers_dataset, top_k=top_k, faiss=faiss\n",
    "        )\n",
    "        _, reranked_indices = rerank(query, retrieved_indices, answers)\n",
    "\n",
    "        if relevant_index in reranked_indices:\n",
    "            rank = reranked_indices.index(relevant_index) + 1\n",
    "            reciprocal_rank = 1 / rank\n",
    "        else:\n",
    "            reciprocal_rank = 0\n",
    "\n",
    "        mrr_total += reciprocal_rank\n",
    "\n",
    "    mrr = mrr_total / len(queries)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "mrr_rerank_semantic_search = calculate_mrr_rerank(\n",
    "    subset_questions, questions, answer_embeddings, top_k=5\n",
    ")\n",
    "print(\"Mean Reciprocal Rank (MRR) with Reranking (Cosine Search):\", mrr_rerank_semantic_search)\n",
    "\n",
    "mrr_rerank_faiss = calculate_mrr_rerank(\n",
    "    subset_questions, questions, answer_embeddings, top_k=5, faiss=True\n",
    ")\n",
    "print(\"Mean Reciprocal Rank (MRR) with Reranking (FAISS Search):\", mrr_rerank_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'bielik',\n",
       "   'full_name': 'speakleash/Bielik-11B-v2.2-Instruct',\n",
       "   'name': 'speakleash/Bielik-11B-v2.2-Instruct'},\n",
       "  {'id': 'cohere',\n",
       "   'full_name': 'CohereForAI/c4ai-command-r-plus',\n",
       "   'name': 'CohereForAI/c4ai-command-r-plus'},\n",
       "  {'id': 'llama3.1-8b',\n",
       "   'full_name': 'meta-llama/Llama-3.1-8B-Instruct',\n",
       "   'name': 'meta-llama/Llama-3.1-8B-Instruct'},\n",
       "  {'id': 'llama',\n",
       "   'full_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
       "   'name': 'meta-llama/Meta-Llama-3.1-70B-Instruct'},\n",
       "  {'id': 'llama-guard',\n",
       "   'full_name': 'meta-llama/Llama-Guard-3-8B',\n",
       "   'name': 'meta-llama/Llama-Guard-3-8B'},\n",
       "  {'id': 'llama3.1',\n",
       "   'full_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct',\n",
       "   'name': 'meta-llama/Meta-Llama-3.1-70B-Instruct'},\n",
       "  {'id': 'openchat',\n",
       "   'full_name': 'openchat/openchat-3.5-1210',\n",
       "   'name': 'openchat/openchat-3.5-1210'},\n",
       "  {'id': 'mixtral-8x22B',\n",
       "   'full_name': 'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
       "   'name': 'mistralai/Mixtral-8x22B-Instruct-v0.1'}]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL = \"https://services.clarin-pl.eu/api/v1/oapi\"\n",
    "MODELS_ENDPOINT = f\"{BASE_URL}/models\"\n",
    "CHAT_ENDPOINT = f\"{BASE_URL}/chat/completions\"\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "response = requests.get(MODELS_ENDPOINT, headers=headers)\n",
    "models = response.json()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/imdb_dataset.pkl\"):\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"embeddings\": normalize((model.encode(x[\"text\"], convert_to_tensor=True)).cpu())}\n",
    "    )\n",
    "    dataset = dataset.add_faiss_index(column=\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "    with open(\"../data/imdb_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    dataset.save_faiss_index(\"embeddings\", \"../data/imdb_texts.faiss\")\n",
    "else:\n",
    "    with open(\"../data/imdb_dataset.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(api_token, query, rag_context=None):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    if rag_context:\n",
    "        text_context = \"\\n\".join(rag_context)\n",
    "        prompt = f\"Answer the question based only on the following context:\\n{text_context}\\nQuestion: {query}.\"\n",
    "    else:\n",
    "        prompt = query\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"bielik\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 160,\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "\n",
    "    response = requests.post(CHAT_ENDPOINT, headers=headers, json=data)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "What is the best movie of all time?\n",
      "\n",
      "Answer from LLM memory: \n",
      "Determining the \"best\" movie of all time is subjective and depends on personal preferences. However, some films like \"The Godfather,\" \"Citizen Kane,\" \"Pulp Fiction,\" \"Shawshank Redemption,\" and \"The Wizard of Oz\" are often cited in lists of the greatest films of all time. \"Citizen Kane\" frequently tops many professional rankings.\n",
      "\n",
      "Answer with RAG: \n",
      "Based on the provided context, the best movie of all time is \"Return Of The Jedi.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the best movie of all time?\"\n",
    "answer = generate_answer(api_key, query)\n",
    "print(f\"Question: \\n{query}\\n\")\n",
    "print(f\"Answer from LLM memory: \\n{answer}\\n\")\n",
    "\n",
    "scores, indices = search(query, dataset[\"embeddings\"], dataset, top_k=5, faiss=True, answers_key=\"text\")\n",
    "rag_context = [dataset[idx][\"text\"] for idx in indices]\n",
    "answer = generate_answer(api_key, query, rag_context)\n",
    "print(f\"Answer with RAG: \\n{answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
